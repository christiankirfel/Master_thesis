\chapter{Adversarial Neural Network}
\label{chp:ANN}

In this chapter the setup and training of the Adversarial Neural Network is described. The Network did not achieve the desired results in its initial configuration.
For this reason different configurations for the implementation of the network structure are presented in addition to the hyperparamter investigations.
The first part of the chapter focuses on the initial run of the ANN using the base network presented in chapter~\ref{chp:simple_NN}. The results are investigated and the hyperparameters are adapted using an initial setup for the second network.
The second network is investigated trying out a number of significantly different setups.
The problems with the initial setup are discussed and used to motivate a different approach adapting the network input which is then also investigated.
Lastly the effectiveness of the two models is discussed presenting possible future steps as the training and setup were strongly limited by the time constraint of this work.
As in chapter~\ref{chp:simpleNN} the final setup is presented first and its hyper-parameters are then step-wise motivated. 
\section{Prerequisites}

The technical details for the setup are the same as for the classifying network as introduced in section~\ref{sec:technicals}.
The adversarial setup uses the classifier trained and tuned in chapter~\ref{chp:simple_NN} as a basis. For the classical setup this output is used as input for the adversarial network.
To tune the hyperparameters of the second network the classifier was left unchanged at first. Later on the internal hyper-parameters of the optimiser were adapted to improve the overall result.



\section{Setup of the classical adversary}

\subsection{Choice of the adversary's architecture}

\subsection{Choice of the optimisation}

As for the classifier SGD and Adam were tested for the adversarial neural network. Adam is shown for its bad performance. SGD is tested in full detail.

put adam in appendix

\section{Alternate setups of the adversary}

With the initial setup suggested in the adversary kept on being succesful at its task while the classifier did not get significantly less dependent on systematics. A possible assumption one can make is that the information provided by the last node is not sufficient to make the influence of the adversary count. Alternatively the insentive provided by the optimiser might not be iterated effectively. Therefore, instead of using the final layer of the classififer as input the last hidden layer is used. Alternatively a further hidden layer is added that contains less nodes than the final hidden layer to balance the information.

\subsection{Uncompressed input}

\subsection{Compressed input}
