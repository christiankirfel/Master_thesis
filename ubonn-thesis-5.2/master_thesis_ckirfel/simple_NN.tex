\chapter{Hyperparameter optimisation of a classifying neural network}
\label{chp:simpleNN}

The basis of the adversarial neural network is formed by a common neural network trained on signal/background classification the output of which is then given to the adversary creating a negative backfeed for the training.

Before the adversarial, second network is added, the classifying network is optimised on its own to make sure that its setup is sufficient for the classification task.
During the adversarial training this setup can be updated if the structure is not optimised for the additional task of a model independent of systematics.

This chapter describes the hyperparameter optimisation of the first network starting with the motivation  of the input information.
The second section explains the choice of the architecture followed by the step-wise setup of the optimiser.
Lastly regularisation of the network is tested and described.
The overall aim of the description is to provide some understanding on the parameters available and their correlations and impact on the model to not only use these insights for the adversarial training but also to motivate possible problem solutions later on.


To train the network and to test its performance all its parts need to be in place. Therefore all hyperparameters had to be initialized with values assumed to be around a good setup. Initially this was accomplished by starting with a very simple network using a minimal set of hyperparamters which were then elaborated to a more and more optimised network. The results shown in this work are going to be based on the final choice of hyperparamters where only one parameter is then varied to explain the impact of the tuning. For that reason the first section will already introduce the final network structure which is then step by step to be motivated.

The structure was achieved within reason and the computational power constraints.


\section{Technical details}

The artificial neural networks in this thesis were creates using the Keras python library.~\cite{chollet2015keras} 
Keras is an application programming interface written in python and able to run on Tensorflow, CNTK or Theano. It was developed by google and summarizes the necessary calculations for running a deep neural network training in fast and easy modules.
The backend is the package responsible for the underlying vector calculations needed for the network setup and training. In this work the tensorflow packag was used for backend.~\cite{tensorflow2015-whitepaper}

\section{Final setup of the network}

This section describes the fully optimised classifier which is then parameter-wise varied during this chapter to motivate the particular choices.
The hyperparamters are listed and the loss curve, the ROC curve and the final separation are introduced as they are the main tools a training performance is evaluated by in this thesis.


\begin{itemize}
\item Input: \num{14} variables motivated by a BDT variable scan.
\item Hidden layers: \num{6} elu layers $\times$ \num{128} nodes each
\item Output layer: \num{1} sigmoid node
\item Optimisation: SGD, lr $=0.06$, mom $=0.3$, no nesterov, no decay
\item Duration: 600 epochs
\end{itemize}


\section{The input variables}

There were two sets of input parameters tested for the classifier.
The first one is a set of simple kinemativ variables trying to exploit a neural network's ability to deduce all further information from the complete basis.
The second set of variables uses more complex variables based on the most significant variables for a boosted decision tree usage on the same problem.
The variable sets used were:



\begin{table}[!htb]
    \caption{Global caption}
    \begin{minipage}{.5\linewidth}
      \caption{}
      \centering
		\begin{tabular}{l|l}
		Variable                              & Branch                            \\
		\pTmiss                & met                               \\ \hline
		$|\eta_{jet1}|$ & absEta\_Jet1 \\ \hline
		$|\eta_{jet1}|$ & absEta\_Jet2 \\ \hline
		$|\eta_{lep1}|$ & absEta\_Lep1 \\ \hline
		$|\eta_{lep2}|$ & absEta\_Lep2 \\ \hline
		${\pT}_{jet1}$  & pT\_jet1    \\ \hline
		${\pT}_{jet2}$    & pT\_jet2     \\ \hline
		${\pT}_{lep1}$    & pT\_lep1     \\ \hline
		${\pT}_{lep2}$    & pT\_lep2     \\ \hline
		$\phi_{jet1}$   & phi\_jet1    \\ \hline
		$\phi_{jet2}$   & phi\_jet2    \\ \hline
		$\phi_{lep1}$   & phi\_lep1    \\ \hline
		$\phi_{lep2}$   & phi\_lep2    \\ \hline
		\end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \caption{}
		\begin{tabular}{l|l}
		\hline
		Variable & Branch                        \\ \hline
		         & mass\_lep1jet2                \\ \hline
		         & pTsys\_lep1lep2met            \\ \hline
		         & pTsys\_jet1jet2               \\ \hline
		         & mass\_lep1jet1                \\ \hline
		         & deltapT\_lep1\_jet1           \\ \hline
		         & deltaR\_lep1\_jet2            \\ \hline
		         & deltaR\_lep1lep2\_jet2        \\ \hline
		         & mass\_lep2jet1                \\ \hline
		         & pT\_jet2                      \\ \hline
		         & deltaR\_lep1\_jet1            \\ \hline
		         & deltaR\_lep1lep2\_jet1jet2met \\ \hline
		         & deltaR\_lep2\_jet2            \\ \hline
		         & cent\_lep2jet2                \\ \hline
		         & deltaR\_lep2\_jet1            \\ \hline
		\end{tabular}
    \end{minipage} 
\end{table}


As the performance is significantly worse for the set of simple variables the set of complex variables approved by another analysis were chosen for the other tests.



\section{The network architecture}

The architecture of the neural network is formed by its nodes and layers. The choice of the architecture is nontrivial and as a lot of aspect's of machine learning not an exact science.
However, one can make some assumptions about the appropriate architecture.
First of all the complexity of the model should about match the complexity of the task assigned. Although it usually is not trivial to find an estimator for a task's complexity and even less to match it to a certain architecture a test series often leads to a good estimate. Another possible thought is the amount of variables necessary to fully describe a system resulting in the minimum variable number necessary to input in the network which also gives a first estimate on how large the architecture should at least be.
Both the depth and the overall size of the model play a role. A simplified way of describing is by saying that the depth defines how often the input is processed during the network while the number of nodes is the number of features that can be kept during each step of processing.

In general an architecture that is too deep and wide will pick up too many features too fast and overtrains before it gets to a good minimum. This can be seen in an early divergence between the training loss and the validation loss. An architecture too simple is not able to pick up the features of the task at all and is not learning at all. The loss stays constant or changes very slowly.

In this work a testseries was performed, training a network for a wide range combinations of nodes and layers. For the sake of simplicity the number of nodes per layer was kept constant during each training. Two variables were then plotted against the size of the architecture. First the overall smallest loss the model achieved during the training was plotted. The other variable was the minimal difference between the training and the validation loss. To keep it simple the complexity of the architecture was defined as the product of nodes and layers. These are certainly not the most sophisticated indicators for the model's complexity and its performance. Nonetheless the plots show that region of good performance exists which was chosen for this analysis.

The good region lays between about x and y layers and x and y nodes each

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures_simpleNN/minimal_loss.eps}
        \caption{Minimal loss}
        \label{fig:minimal_loss}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures_simpleNN/minimal_diff.eps}
        \caption{Minimal loss difference}
        \label{fig:minimal_diff}
    \end{subfigure}
    \caption{Caption place holder}
    \label{fig:net_complexity}
\end{figure}



\missingfigure{comparehighandlow}

Two different activation functions come to use in the neural network, the main function connects the nodes in the hidden layers while the last one converts the outputto a value between \num{0} and \num{1}. 

\missingfigure{elurelu}

Furthermore the activation function for the hidden layer is elu
the activation function 

\section{Setup of the optimisation}

For the most part of the optimisation SGD was used as the optimiser and its tuning is shown in this section in detail.
Adam was also tested out but not fully tuned due to constraints on tim and computational resources.

\subsection{Choice of the optimiser}

\subsection{Tuning the optimiser}



\section{Regularisation}