\chapter{Machine Learning}
\label{chp:ml}

\section{The concept of machine learning}

Over the last decades, computers have become indispensable tools of science; handling large amounts of data, completing tedious calculations, and controlling sophisticated experiments. For the most part, these  machines were assigned discrete tasks and they followed step-by-step commands, designed beforehand by human users, and had expected outcomes. 
For particle physics in particular, computers have been used to select and process data from large samples, allowing the processing of these data at a speed beyond human capabilities. However, the selection rules always had to be generated by the user, therefore requiring an in depth understanding of the underlying system. In contrast, machine learning enables a program to establish its own decision rules, improving these over several iterations and thereby learning to solve the problem by itself.

There has been a great effort over the last decades trying to implement a way for machines to learn from known quantities. Thereby the machines would be enabled to analyse complex tasks ranging from voice recognition to object classification.
The efficiency and validity of a machine learning model is highly dependent on the human understanding of the problem at hand. One prerequisite for a successful model is the tuning of the degrees of freedom and parameters to the complexity of the assignment. This is called \emph{hyperparameter optimisation}, a task often proportional to the learning process itself.


Machine learning can be exemplified by drawing an analogy to human beings. In order to solve a problem, the machine needs to understand the system, to evaluate a decision step, and finally generate new decision steps.
Understanding a system means to be aware of all its features and possibilities. Humans have their senses to easily break down observations into useful features and concepts that can then be processed for finding a decision. A computer has no such senses, and for most tasks, this means that the step of filtering information for a relevant subset of features has still to be done by humans or a good preprocessing algorithm.
Once a system has been converted to a subset of features usable by a computer, the step of making its own decisions has to be implemented. This can be done by weighting and interconnecting the information using structures inspired by neurons and synapses in the human brain. 


The structure and complexity of the network enables it to learn from data. In addition, a metric is introduced that measures the quality of the model, called the \emph{cost} or \emph{loss function}. This function allows for iterative improvement as a decrease in its value is considered an improvement by the network. Combined with an optimiser, which suggests further steps, this function is a basis for a network to independently approach a good decision rule for a incompletely investigated topic.

A very commonly used machine learning technique is the\emph{artificial neural network} which, on its own forms, an extended field that builds the base of this work. The essential concepts of machine learning will be explained in the context of neural networks.

\section{Neural Networks}

The artificial neural network, or just neural network, is one of the most commonly applied approaches to machine learning. Its structure and naming is inspired by the neurons forming the human nervous system.

Instead of living neurons, a neural network consists of numerous very simple processors, called nodes.
 These nodes are usually structured into several layers as presented in figure\todo{todo}. In addition, there are several ways to structure and connect these nodes, frequently matching a certain problem. In this explanation, only the most commonly way is explained. In that case each node of a middle layer is getting input from each node in its predecessor and is outputting to each node of the following layer. This obviously is only true in one direction for the extreme layers, input and output. A simple view of communication between nodes is shown in figure \ref{fig:nodes_nomenclature} describing the step between a node and the previous layer. This is called a feed-forward neural network, the math of which will be described in detail later.

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.18]{figures_ML/nodes_nomenclature.eps}
	\caption[Network parameter nomenclature]{Network propagation from layer $(L-1)$ to layer $L$}
	\label{fig:nodes_nomenclature}
\end{figure}


\subsubsection{The input layer}

To understand a task and draw reasonable conclusions, the underlying system has to be understood at first, which means its features need to be determined and summarised. The human brain is capable of investigating unknown systems and learning the features that are the most unique or interesting ones. For that, the nervous system uses its senses to explore the system and process them later. To allow a machine to do something similar, the unknown system has to be represented in a way that it is clear for the network, what to look out for. This usually is the task that requires most preprocessing by the user. The simplest case is to submit a list of variables to the network. In particle physics, this could be kinematic variables of the final reconstructed objects in an event.

The input to a neural network is given to the input layer of nodes and subsequently processed through each following layer. For diverse tasks, different layers might deal with various parts of the information. However, in this work the linear way of giving all information used to an input layer and then processing it is used.

\subsubsection{Decision making process}

Computers representing nothing more than very powerful calculators, excel at performing high numbers of clearly defined calculations. This requires a precisely elaborated task containing no uncertainty.
This is completely different for the human nervous system, which relies on a certain uncertainty when processing information through a net of neuron cells. In this net, the output of every neuron is taken as input for the surrounding neurons. The challenge of machine learning is to represent this fuzziness by many, somewhat discrete calculations. In a neural network, the neurons and their fuzzy interactions are represented by the nodes. Like neurons, each node can use input from many other nodes to create a new output signal. Thereby the sets of input information can be linked to each other in numerous ways. Combined with a weighting system, this allows one to create complex models and match a variety of problems.

\begin{align}
    z_j^L = \sum_{k=0}^{N} \omega_{jk}^L a_k^{L-1} + b_k
    \label{eq:node_input}
\end{align}

The input of every node is the weighted output of all previous nodes, as shown in equation~\eqref{eq:node_input}. $z_j^L$ is the input to the $j$-th node in the $L$-th layer. $\omega_{jk}^L$ is the weight from the $k$-th node in the previous layer to this node, $a_k^{L-1}$ is that node's output, and $b_k$ the relevant bias; representing a possible intercept of the functionality. The sum indicates that all $k$ previous nodes contribute to the input to the $j$-th node.


The weight allows a network to predict which variables are correlated or allow for better decision rules when combined. In addition, the weights can just define the strongest variables and features. Furthermore, the output of each node is a non-linear combination of the input. This means the output can range from just one and zero to an exponential function. This is called the \emph{activation-function} of a node. A common choice is the sigmoid function as presented in equation \eqref{eq:sigmoid_activation}.~\cite{chollet2015keras}

\begin{align}
    a_j^L = \sigma ( z_j^L ) = \frac{1}{1 + e^{z_j^L}}
    \label{eq:sigmoid_activation}
\end{align}

The sigmoid function has an output between \num{0} and \num{1}, which is frequently desired for nodes. Especially for the final layer, as we are looking for predictions of outcome probabilities. A selection of further activation functions is shown in table \ref{tab:activation_functions}.

\begin{table}[]
\centering
\begin{tabular}{l|l|l}

Name                    & Function & Plot \\ \hline
Sigmoid                 &$f(x) = \frac{1}{1 + e ^{-x}}$        &   \raisebox{-0.5\height}{\includegraphics[scale=0.3]{figures_ML/sigmoid}}              \\ \hline
Tangens Hyperbolicus    &$f(x) = \frac{2}{1 + e ^{-2x}} -1$        &        \raisebox{-0.5\height}{\includegraphics[scale=0.3]{figures_ML/tanh}}         \\ \hline
Rectified Linear Unit, \textsc{ReLu}   &$f(x) =
  \begin{cases}
    0       & \quad \text{if } x < 0\\
    x  & \quad \text{if } x \geq 0
  \end{cases}$     &      \raisebox{-0.5\height}{ \includegraphics[scale=0.3]{figures_ML/relu}}          \\ \hline
Exponential Linear Unit, \textsc{ELU} &$f(x) =
  \begin{cases}
    \alpha ( e^x -1 )     & \quad \text{if } x < 0\\
    x  & \quad \text{if } x \geq 0
  \end{cases}$      &        \raisebox{-0.5\height}{\includegraphics[scale=0.3]{figures_ML/elu}}         \\ 
\end{tabular}
\caption{Selection of activation functions taken from the Keras documentation. \cite{chollet2015keras}}
\label{tab:activation_functions}
\end{table}

For this thesis, mainly the exponential linear unit, or \textit{elu}, and the rectified linear unit, or \textit{relu}, were tested.
\textit{Elu} is suitable for converging the cost to zero relatively fast and provides the possibility of negative output while \textit{relu} allows for the same benefit as sigmoid but  requires less computational power for the simply linear output for positive values.

Of course the network does still not know the task assigned to it, but if we add a cost function to estimate the quality of a decision rule created by a certain combination of the input, we can easily make the network approach a better model in each step. Following the process of cost minimisation, the solution of the initial problem can be approximated.
In this work, the cost function will be called the loss of the model.

In supervised learning, the network is trained with a set of labeled data. Each event in the training set has is assigned a label representing what process the network is looking at. This is referred to as \emph{truth label} or just \emph{label}. Comparing this truth information to the network output makes it very easy to calculate the loss as the deviation of the network output from the known label. A possible loss function is the \emph{crossentropy} or just \emph{binary crossentropy} for a binary output result. As in this work the result is binary, signal and background, the binary crossentropy is the natural choice of loss function. Equation \eqref{eq:binary_crossentropy} shows the underlying function where $p$ is the estimated probability for the prediction $\hat{y}$ and $y$ is the truth label.

\begin{align}
    C = -(y \log p + (1 - y) \log (1 - p) ).
    \label{eq:binary_crossentropy}
\end{align}

The loss is the most important indicator for the training quality. This training quality must not be mixed up with the overall quality of generated model. Only after a test on a different sample or real data the model can be fully evaluated.

\subsubsection{Optimisers - Choosing the next step}
\label{sec:optimisation}

The probability interaction of the nodes combined with the loss function enables a network not only to create a model but also to evaluate it. The last missing part is an algorithm that can estimate a step to a model that further minimises the loss. One could certainly do this randomly until the network finds a very low costed decision rule if infinite computational power was provided, but that would neither be an efficient nor the desired learning process.

The output of each node in the final layer is defined by the weighted and biased information of the previous nodes, and lastly the activation function. For one connection, there are three variables that have impact on the loss, the weight, the bias, and the activation. Summarising this information for all nodes in a vector defines the \emph{loss-vector}. The gradient of this vector is an estimator for the impact of each parameter on the overall loss and thereby gives a preferred direction for the model. Updating a network's parameters based on this gradient is called \emph{backpropagation}. The algorithm works as follows:

\begin{enumerate}
    \item A certain set of input variables is iterated through all layers of a network resulting in an estimator $\hat{y}$ at each output node.
    \item The sum of deviations from the true value at all output nodes is determined as the loss $C$ of the setup.
    \item The gradient of the loss is calculated as the partial derivative of all network parameters using the following equation:
    \begin{align*}
        \frac{\partial C}{\partial a_k^{L-1}} = \sum_{j=1}^N \frac{\partial z_j^L}{\partial a_k^{L-1}} \frac{\partial a_j^L}{\partial z_j^L}\frac{\partial C}{\partial a_j^L}.
    \end{align*}
    \item The parameters are then updated backwards through the layers following the negative loss gradient.
\end{enumerate}
This backpropagation algorithm is the backbone of the neural network's learning process. The decision step based on the gradient defined above is specified by the networks optimizer and deserves a bit more attention.

 There are different choices of optimisers, which try to accommodate different problems as well as some important parameters to tune for an effective training. The length of a learning step has to match the problem's topology to properly let the model converge. First, we define the gradient, $g$, in a more general way. The batch size, $m$, stands for the amount of data processed to evaluate the next step. $f$ is the network for a current configuration or model, $\theta$, and the prediction $\hat{y}$. $\theta$ summarises all the parameters optimised by the network during the training. The truth label is $y$. Using these definitions the gradient becomes:
 %
\begin{align}
    g = \frac{1}{m} \nabla_{\theta} \sum_j L(f(\hat{y}^j; \theta), y^i).
\end{align}
%
The configuration $\theta$ is then updated using the gradient and a multiplicative constant, $\eta$, called the \emph{learning rate}, which determines the step size for each update following:
%
\begin{align}
    \theta \prime = \theta - \eta g.
\end{align}
%
Optimisation processes like these are gradient descent based optimisers and can be considered the basis of all optimisers. Depending on the choice, they might be based on the whole training sample or just a mini batch of the sample. The most basic form has only learning rate as its hyperparameter. A good learning rate should be small enough to avoid oscillations around minima but high enough to approach a minimum efficiently. A good estimate is given by the Robbins Monro condition:
%
\begin{align}
    \sum_k \eta_k = \infty,\\
    \sum_k \eta_k^2 < \infty.
\end{align}

As the choice of learning rate will not be perfect for every part of the problem's topology, \emph{momentum}, $\nu$, can be introduced as a second parameter to the optimiser~\cite{chollet2015keras}. The effect desired is twofold; momentum should increase the learning rate in the direction of the minimum and lower it when approaching said minimum. Momentum scales each step by how aligned previous steps were, meaning it will allow avoiding local minima or moving slowly along a slope. This is accomplished by enlarging steps at the beginning of the training but diminishing them at the end close to the minimum. It promises to speed up the training with less risk of large oscillations, which is an effect of high learning rates. Momentum also takes a single scaling hyperparamter $\alpha$ and is updated each step in the following way: 
%
\begin{align}
    \nu^{\prime} = \alpha \nu - \eta \frac{1}{m} \nabla_{\theta} \sum_j L(f(\hat{y}^j; \theta), y^j),\\
    \theta^{\prime} = \theta + \nu^{\prime}.
\end{align}
%
Alternatively one can use \emph{Nesterov momentum}~\cite{chollet2015keras}, which is a more advanced adoption of momentum as it updates the step a further time after applying the gradient: 
%
\begin{align}
    \nu^{\prime} = \alpha \nu - \eta \frac{1}{m} \nabla_{\theta} \sum_j L(f(\hat{y}^j; \theta + \alpha \times \nu), y^j),\\
    \theta^{\prime} = \theta + \nu^{\prime}.
\end{align}
%
Finally, it can be helpful to decrease the learning rate of the network stepwise while approaching a minimum to avoid oscillations or even missing the minimum completely. This can be accomplished by the hyperparameter of learning rate \emph{decay}. It simply decreases the learning rate in each iteration, $t$, by a small hyperparamter, $\phi$, following the assumption that smaller steps are sufficiently close to the minimum~\cite{chollet2015keras}. The learning rate is then defined as:
%
\begin{align}
    \eta^{\prime} = \frac{\eta}{1 + \phi t}.
\end{align}
%
\subsubsection{Adaptive optimisers}

In addition to the merely gradient based optimisers, there are \emph{adaptive optimisers}. Learning rate and momentum as previously described are difficult to tune to every part of the training process as the topology of the problem might rapidly change. Therefore adaptive optimisers update their parameters based on the training process. From all of the adaptive optimisers~\cite{chollet2015keras} \emph{Adam} is probably the most popular~\cite{2014arXiv1412.6980K}. \emph{Adam} updates both its learning rate and momentum over the course of the training based on an exponentially decaying average of past gradients and squared gradients. The average makes sure that the parameters keep getting updated based on past steps. They should be decaying, as otherwise, the parameters would rapidly shrink. The decay of the averages is defined by a hyperparameter $\beta$ resulting in this gradient definitions:
%
\begin{align}
    \hat{g}^2 = \frac{\sum g^2}{1 - \beta_1^t},
\end{align}
%
\begin{align}
    \hat{g} = \frac{\sum g}{1 - \beta_2^t}.
\end{align}
%
The model's parameters are then updated according to:
%
\begin{align}
    \theta^{\prime} = \theta + \frac{\eta}{\sqrt{\hat{g}^2} + \epsilon} \hat{g}.
\end{align}
%
\emph{Adam} is often considered an excellent algorithm as it contains many corrections to hyperparamters during the training, and thereby allows for easier optimisation. However, it also needs more computational power.


\section{Regularisation and Optimisation}

Fluctuations and noise in the training sample can be a big problem for a model trained on the sample. A neural network might pick noise and random fluctuations up as features of its decision rule which basically is the process of \emph{overfitting}.
The network observes way more features to work with than those actually present in reality or even in a different fraction of the sample used by the training. The most extreme scenario is that the network is large and deep enough to pick up every single feature in the training sample. If that happens, the training error becomes very low and indicates a very good decision rule. For a different sample this decision rule is at most very unreliable but probably strictly wrong resulting in a high test sample error. The network just picked up and remembered every single feature in the training sample instead of general correlations and thus becomes a mask of the sample.

A possible way to solve this issue is stopping the training early or finding an elaborate estimator to stop on. This way, noisy features would not yet be part of the discriminant. This might in turn also lead to a suboptimal result of the overall training as one cannot be sure that the correct features always get included first.

More sophisticated approaches are called \emph{regularisations} of a neural network. The most commonly used solution is a so called \emph{dropout} layer described in the following subsection. Additionally, a \emph{batch normalisation} can have an effect of regularisation and is therefore introduced in this section as well.

\subsection{Dropout}
\label{sec:dropout}

Dropout can be described as an additional layer which attempts to hinder the network from relying on less dominant features. In essence a dropout layer removes different nodes in each iteration. This forces the network to build models that are not based on strong correlations between nodes, making the weights less interdependent. In short, it means training several neural networks depending on which nodes are turned on during a training epoch which it keeps the training in motion for a large number of epochs. Figure~\ref{fig:dropout} sketches the process.

Dropout is added to each layer of a network and can also be restricted to a subset of layers. It slows down the training as the additional motion decelerates the process of finding a minimum. However, it also accelerates each epoch slightly as it simplifies the network architecture. 

\begin{figure}[htbp]
	\centering
	\includegraphics[scale = 0.1]{figures_ML/dropout.eps}
	\caption[Dropout Sketch]{Sketch of network before and after the inclusion of dropout. On the left hand side dropout is not applied and all nodes are connected. On the right hand side the dashed circles are nodes excluded by dropout and therefor not connected to the other nodes.}
	\label{fig:dropout}
\end{figure}



\subsection{Batch normalization}
\label{sec:batch_norm}

In supervised learning, the training result is strongly dependent on the set of data the network is trained on. This means that the performance might change dramatically when the test data is very different. one can imagine a classifier distinguishing between images of cars and apples. If the training set contains predominantly green cars, the colour green might become a strong indicator for the classification \enquote{car}. In general, the colour is not be a defining property of a car and the network will perform slightly worse when trying to classify cars of a different colour. Formally such a change of input is called a \emph{covariance shift}.

A way to reduce the effect of covariance shift is batch normalization. The general output and connections between nodes in a neural network is not necessarily limited, allowing for certain connections to be extremely dominant and overshadowing other features. This is to be avoided as the dominance of some features might just be present in the training sample. This can be achieved by normalizing the output of each layer in the network to the total output and thereby minimising the effects of strongly overrepresented features. This is done by normalizing each output to the mini-batch mean $\mu_B$ and the mini-batch standard deviation $\sigma_B^2$:

\begin{align}
    \mu_B = \frac{1}{m} \sum_i x_i,\\
    \sigma_B^2 = \frac{1}{m} \sum_i (x_i - \mu_B)^2,\\
    x_{i,norm} = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}.
\end{align}





\section{Adversarial Neural Networks}

This main part of this work is the examination and training of an adversarial neural network. An adversarial neural network consists of a classifying network and a second network that tries to regularise the output of the first classifying network.
%
In this section, the concept of an adversarial neural network is motivated and the underlying mathematics as originally stated in paper~\cite{2014arXiv1406.2661G} are presented.
For more information about an approach directly tested on physics, one can refer to "Learning to Pivot with Adversarial Networks"~\cite{Louppe:2016ylz}.

\subsection{The adversarial neural network}

Neural networks have been very successful in classification tasks but not so much for generative tasks. This was the original problem that gave birth to the idea of a generative adversarial network. Unfortunately generative networks often produce output that is very easy to distinguish from real samples. The solution suggested is adding a classifier that tries to distinguish between generated samples and real samples. As long as this adversary is able to accomplish this task, the first network fails at its generative task. Training the two networks against each other disincentivises the generative network from using the features not dominant in real samples.

In this work, the first network is not a generator but a classifier separating signal events from background events in a Monte Carlo simulation. These simulations contain systematic uncertainties and different samples represent a set of plausible data generation processes. The classifier should not be too dependent on variables with high systematic uncertainties as the network cannot account for differences in the training and testing sample or even in real data. If the classifier has these strong dependencies on systematic uncertainties it might lead to a high covariance shift.

Instead of a generated sample and a truth sample, a so called nominal sample and systematic samples are used as input for the second network. Systematic samples have slightly different distributions than the original samples because of changes to the variables with the systematic uncertainties. Training the second network on determining whether it is looking at a nominal or a systematic sample allows to estimate how strongly the model depends on variables with high systematic uncertainties. Training the classifier against the adversarial network promises to reduce the effect of systematic uncertainties on the model. If the topology of the problem allows for it, this should render the model generated by the classifier pivotal. That means it does not depend on the unknown values of the nuisance parameters.

Mathematically, this comes down to a minimax decision rule or a competition between two neural networks. The classifier is referred to as $Net1$ and the adversary as $Net2$ and the problem becomes:
%
\begin{align}
    \min_{Net1} \max_{Net2} V(Net1, Net2) = \mathbb{E}_{\mathit{x} \sim \rho_{data}} [ log Net1(\mathit{x}) ] + \mathbb{E}_{\mathit{z} \sim \rho_{sys}} [ log (1 - Net2(\mathit{z}) ) ].
\end{align}
%
$V(Net1, Net2)$ is the combined value function for the two adversary networks. The first network is trained to be an optimal classifier represented by $log Net1(\mathit{x})$ while the second network is trained to distinguish between the nominal and systematics distribution $\mathit{z} \sim \rho_{sys}$ represented by $log (1 - Net2(\mathit{z}))$.

 In theory, the first classifier should be trained slowly and kept close to its optimum while the second network slowly learns and allows the first network to adapt to it. This is achieved by training the two networks successively over multiple iterations using a combined value function.
A combined loss function is used as the value function. It is just the difference between the two separate loss functions with a hyper-parameter, $\lambda$, to control the impact of the adversary as shown in equation \eqref{eq:adversarial_loss}:
%
\begin{align}
    \mathcal{L} = L_{net1} - \lambda L_{net2}.
    \label{eq:adversarial_loss}
\end{align}
%
In the first step of each iteration, the first network is trained using the combined loss function, $\mathcal{L}$. In the second step, the adversary is trained using its simple loss function $L_{net2}$. Each of the networks has the usual set of hyper-parameters to optimise explained in detail in the previous sections

For this thesis, the adversarial network is set up by building a classifying network. The information of the classifier is then fed into both the classifier's output layer and the adversarial network creating a second model based on the first network's model. The networks are then trained successively controlling the combined and separate losses. Figure \ref{fig:ANN_sketch} shows a sketch of the setup.
%
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{figures_ML/ANN_sketch.png}
	\label{fig:ANN_sketch}
	\caption[Adversarial setup sketched]{Sketch of the network setup. The middle part is the classifier. The model generated is fed into both the second network and the classifier's output. This way the two losses $L_{net1}$ and $L_{net2}$ are generated. Furthermore the training of the classifier immediately affects both output models.}
\end{figure}
%
The hyperparameter $\lambda$ is set to tune the impact of the second network on the model. A large $\lambda$ leads to a very pivotal model but can also decrease the overall quality of the classifier.

In order to better understand the figures shown in chapter \ref{chp:ANN}, the three loss values of an adversarial neural network will be introduced as the last part of this chapter.
The first loss belongs to the classifier and is expected to first increase as the adversary finds a good model and to decrease once a more pivotal model is found. 
The second loss displays the adversary's performance ideally decreasing at first to then increase and saturate as a pivotal model renders it impossible to extract any information.
Lastly the combined loss is displayed showing the overall performance and decreasing as good models are found for both the classifier and the adversary.
Figure \ref{fig:losses_paper} shows an example for these loss functions taken from the paper "Learning to Pivot with Adversarial Networks"~\cite{Louppe:2016ylz}.
%
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures_ML/losses_paper}
	\caption[Exemplary loss of an adversarial network structure]{Three losses of an adversarial network for $\lambda = 10$ taken from a toy example~\cite{Louppe:2016ylz}. From top to bottom the classifier, adversary and combined loss is presented. $T$ is the number of training iterations.}
	\label{fig:losses_paper}
\end{figure}



