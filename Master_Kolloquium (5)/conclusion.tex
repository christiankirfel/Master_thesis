\begin{frame}{Thoughts}
    \begin{block}{Input}
        \begin{itemize}
            \item Different variable set $\rightarrow$ different problem topology
            \item Concatenate layers as input $\rightarrow$ Combination of more information and final output
            \item Separate background from the adversary's input $\rightarrow$ remove possible bias
        \end{itemize}
    \end{block}
    %
    \begin{block}{General}
        \begin{itemize}
            \item Different analysis $\rightarrow$ Insight on the network's general inability
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Conclusions}
    \begin{block}{Optimisation of a combination of networks}
        \begin{itemize}
            \item An adversary cannot just be added
            \item An ANN has to be set up as a combination
        \end{itemize}
    \end{block}
    %
    \begin{block}{Input of the adversary}
        \begin{itemize}
            \item Input of the adversary does not justify the architecture
            \item Space for improvement in the input
        \end{itemize}
    \end{block}
    %
    \begin{block}{Strong dependency on the learning rate}
        \begin{itemize}
            \item Learning rate is the defining factor of the performance of the losses
            \item Presumably due to the fine topology around the minimum
        \end{itemize}
    \end{block}
\end{frame}

%\begin{frame}{er}
%    \begin{figure}
%        \centering
%        \includegraphics[width=\textwidth]{xkcd}
%        \caption{https://xkcd.com/1838/}
%    \end{figure}
%\end{frame}