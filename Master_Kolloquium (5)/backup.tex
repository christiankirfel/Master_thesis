


\begin{frame}{Neural Networks}
\begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{itemize}
        \item Built of layers of simple processors called nodes
        \item Each node is connected to each node in the surrounding layers
        \item The connection is built by a linear function with a weight $\omega$ and a bias $b$ 
        and a non linear activation function $\sigma$
        \item Learning is accomplished by backpropagation and a step optimizer
    \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
    \begin{figure}
        \centering
        \includegraphics[scale=0.05]{network.eps}
        \label{fig:my_label}
    \end{figure}
    \vspace{0.3cm}
    \begin{center}
     $z_j^L = \omega_{jk}^L a_k^{L-1} + b_k$\\
        $a_j^L = \sigma ( z_j^L )$
    \end{center}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}{Neural Networks}
\begin{figure}
    \centering
    \includegraphics[scale=0.15]{nodes_nomenclature.eps}
    \caption{Forward Propagation in a neural network}
    \label{fig:my_label}
\end{figure}
\end{frame}

\begin{frame}{Choosing the next step}
    \begin{itemize}
        \item In supervised learning truth tagged training data allows to calculate a cost function to estimate a model's quality. For example crossentropy:
        \begin{align}
            C = -(y \log p + (1 - y) \log (1 - p) )
            \label{eq:binary_crossentropy}
        \end{align}
        \item Backpropagation estimates the parameters' impact on the cost function using the partial derivatives
        \begin{align*}
            \frac{\partial C}{\partial a_k^{L-1}} = \sum_{j=1}^N \frac{\partial z_j^L}{\partial a_k^{L-1}} \frac{\partial a_j^L}{\partial z_j^L}\frac{\partial C}{\partial a_j^L}
        \end{align*}
        \item Each parameter is then updated accoring to its impact on the cost function
    \end{itemize}
\end{frame}


\begin{frame}{Adversarial Neural Networks}
    \begin{itemize}
        \item Originally introduced as Generative adversarial neural networks to overcome weaknesses of generative networks \cite{2014arXiv1406.2661G}
        \item Adds a second network controlling the dependency on features with large systematic uncertainties. 
        \item The adversarial structure of the networks creates a minimax game
        \item Combined loss function
    \end{itemize}
\end{frame}

\begin{frame}{Optimisers}
\begin{itemize}
    \item Gradient Descent: Update parameters in the opposite direction of the error gradient
    \item Stochastic Gradient Descent: Gradient Descent but for each training example separately
    \item Adaptive Moment Estimation, Adam: adaptive learning rate based on exponentially decaying mean of past gradients and past square gradients.
    \item Others
    \item https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f
\end{itemize}
    
\end{frame}



\begin{frame}{Learning rate, momentum, decay}
\begin{itemize}
    \item Learning rate: Step-length in gradient direction
    \begin{align}
    \theta^{\prime} = \theta - lr \cdot g
    \end{align}
    \item Momentum: create an adaptive and weight dependent learning rate
    \begin{align}
    \nu^{\prime} = \alpha \nu - \epsilon \frac{1}{m} \nabla_{\theta} \sum_j L(f(\hat{y}^j; \theta), y^j)\\
    \theta^{\prime} = \theta + \nu
    \end{align}
    \item Decay: Decrease the learning rate over the course of the training
    \begin{align}
    \epsilon^{\prime} = \frac{\epsilon}{1 + \phi t}
    \end{align}
\end{itemize}
\end{frame}


\begin{frame}{Dropout}
\begin{figure}
    \centering
    \includegraphics[scale=0.07]{dropout.eps}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
\end{frame}

\begin{frame}{Technical details}
    \begin{itemize}
        \item The neural networks were built using Keras \cite{chollet2015keras} with tensorflow \cite{tensorflow2015-whitepaper} as backend
    \end{itemize}
\end{frame}

\begin{frame}{Variables}
\begin{table}[h]
    \begin{minipage}{.5\linewidth}
      \centering
		\begin{tabular}{l}
		Variable         \\ \hline
		\pTmiss          \\ \hline
		$|\eta_{jet1}|$  \\ \hline
		$|\eta_{jet1}|$  \\ \hline
		$|\eta_{lep1}|$  \\ \hline
		$|\eta_{lep2}|$  \\ \hline
		${\pT}_{jet1}$   \\ \hline
		${\pT}_{jet2}$   \\ \hline
		${\pT}_{lep1}$   \\ \hline
		${\pT}_{lep2}$   \\ \hline
		$\phi_{jet1}$    \\ \hline
		$\phi_{jet2}$    \\ \hline
		$\phi_{lep1}$    \\ \hline
		$\phi_{lep2}$    \\ \hline
		\end{tabular}
		\caption{Simple kinematic variables}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
		\begin{tabular}{l}
		Variable                   \\ \hline
		$m_{lep1} + m_{jet2}$      \\ \hline
		${\pT}_{lep1} + {\pT}_{lep2} + \pTmiss     $     \\ \hline
		${\pT}_{jet1} + {\pT}_{jet2}    $     \\ \hline
		$m_{lep1} + m_{jet1}$      \\ \hline
		${\pT}_{lep1} - {\pT}_{jet1}$\\ \hline
		${R}_{lep1} - {R}_{jet2}$    \\ \hline
		$R_{lep1, lep2} - R_{jet2}$       \\ \hline
		$m_{lep2} + m_{jet1}$        \\ \hline
		${\pT}_{jet2}$               \\ \hline
		${R}_{lep1} - {R}_{jet1}$	 \\ \hline
		${R}_{lep1} - {R}_{jet1}$	 \\ \hline
		${R}_{lep2} - {R}_{jet2}$	 \\ \hline
		$\text{Centrality}_{lep2} + \text{Centrality}_{jet2}$ \\ \hline
		${R}_{lep2} - {R}_{jet1}$    \\ \hline
		\end{tabular}
		\caption{Complex variables}
    \end{minipage} 
\end{table}
\end{frame}